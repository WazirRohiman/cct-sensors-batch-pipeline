# System Architecture

## Overview

The City of Cape Town Environmental Sensors Data Pipeline implements a robust, production-ready batch processing system using containerized Apache Airflow orchestration and DuckDB analytics storage.

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Ingestion      â”‚    â”‚   Processing    â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Air Quality â”‚ â”‚â”€â”€â”€â–¶â”‚ â”‚ ArcGIS       â”‚ â”‚â”€â”€â”€â–¶â”‚ â”‚ Staging     â”‚ â”‚
â”‚ â”‚ (2016-2022) â”‚ â”‚    â”‚ â”‚ Fetch Logic  â”‚ â”‚    â”‚ â”‚ Pipeline    â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â”‚              â”‚ â”‚    â”‚ â”‚ (Future)    â”‚ â”‚
â”‚                 â”‚    â”‚ â”‚ â€¢ Retry      â”‚ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”‚ â€¢ Validate   â”‚ â”‚    â”‚                 â”‚
â”‚ â”‚ Wind Data   â”‚ â”‚â”€â”€â”€â–¶â”‚ â”‚ â€¢ Stream     â”‚ â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ (2016-2020) â”‚ â”‚    â”‚ â”‚ â€¢ Quarantine â”‚ â”‚    â”‚ â”‚ Normalise   â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â”‚ (Future)    â”‚ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â–¼
                       â”‚   Storage       â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚                 â”‚    â”‚   Analytics     â”‚
                       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚                 â”‚
                       â”‚ â”‚ Raw Files   â”‚ â”‚â—€â”€â”€â”€â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                       â”‚ â”‚ data/raw/   â”‚ â”‚    â”‚ â”‚ DuckDB      â”‚ â”‚
                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â”‚ Star Schema â”‚ â”‚
                       â”‚                 â”‚    â”‚ â”‚ (Future)    â”‚ â”‚
                       â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                       â”‚ â”‚ Quarantine  â”‚ â”‚    â”‚                 â”‚
                       â”‚ â”‚ data/quar./ â”‚ â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                       â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â”‚ Query       â”‚ â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚ Interface   â”‚ â”‚
                                              â”‚ â”‚ (Future)    â”‚ â”‚
                                              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Data Flow Pipeline

### âœ… Phase 3: Data Ingestion (Current)

**Step 1: Source Identification**
- Configuration-driven source discovery via `sources.yaml`
- Support for multiple URL types (direct data URLs, ArcGIS item pages)
- Dynamic task generation in Airflow DAGs

**Step 2: HTTP Ingestion**
```
ArcGIS Portal â†’ fetch_item() â†’ data/raw/
             â†“
      Retry Logic (3x)
             â†“  
      Content Validation
             â†“
      Success â†’ Raw Files
             â†“
      Failure â†’ Quarantine
```

**Step 3: File Organization**
- **Raw Storage**: `data/raw/air_quality_YYYY.[xlsx|zip]`, `data/raw/wind_YYYY.xlsx`
- **Error Handling**: `data/quarantine/[source]_YYYY_failed.txt`
- **Logging**: Container logs + Airflow task logs

### âœ… Phase 4: Data Processing COMPLETE

**Step 4: Staging Pipeline (Wind + Air Quality)**
```
data/raw/wind_YYYY.xlsx â†’ Multi-header parsing + station surrogate keys
                       â†’ Validation (ranges, timestamp coercion)
                       â†’ data/staged/wind_YYYY.parquet (5 files)

data/raw/air_quality_YYYY.[xlsx|zip] â†’ ZIP extraction + Excel parsing
                                    â†’ Station surrogate key mapping
                                    â†’ data/staged/air_quality_{pollutant}_{year}.parquet (19 files)
```

**Step 5: Schema Normalization (Wind + Air Quality)**
```
data/staged/*.parquet (24 files)
          â†“
Wideâ†’Tall transformation with unit + quality flags
          â†“
Join station metadata from station_mapping.yaml
          â†“
data/normalised/*.parquet (24 normalised files)
â”œâ”€â”€ wind_YYYY_normalised.parquet (5 files)
â””â”€â”€ air_quality_{pollutant}_{year}_normalised.parquet (18 files)
          â†“
Idempotent station_dim.parquet update (11 stations, PKs 1-11)
```

**Step 6: Database Loading (Next - Phase 5)**
```
Normalised Data (24 fact files + 1 dimension)
               â†“
    DuckDB Star Schema Implementation
               â†“
    dim_station (11 stations, locations, metadata)
               â†“
    fact_measurement (datetime, station_pk, metric, value, quality_flag)
               â†“
    Idempotent Loading (DELETE + INSERT or MERGE pattern)
```

## Infrastructure Components

### Container Orchestration

**Docker Compose Services**:
```yaml
postgres          # Airflow metadata database
airflow-init      # Database migration + user setup + permissions
airflow-webserver # UI and API (port 8080)
airflow-scheduler # Task orchestration
airflow-triggerer # Deferred task handling
```

**Volume Mounts**:
```
./dags â†’ /opt/airflow/dags          # DAG definitions
../data â†’ /opt/airflow/data         # Data storage
../src â†’ /opt/airflow/src           # Pipeline modules  
requirements.txt â†’ /opt/airflow/    # Python dependencies
```

### Data Storage Architecture

**Directory Structure**:
```
data/
â”œâ”€â”€ raw/           # âœ… Downloaded files (24 files: Excel, ZIP)
â”œâ”€â”€ quarantine/    # âœ… Failed downloads + error logs
â”œâ”€â”€ staged/        # âœ… Wide-format Parquet (24 files: 5 wind + 19 air quality)
â”œâ”€â”€ normalised/    # âœ… Tall fact tables (24 files) + station dimension (1 file)
â”œâ”€â”€ duckdb/        # ğŸ”„ Database files (Phase 5)
â””â”€â”€ logs/          # ğŸ”„ Processing logs (Phase 5)
```

**File Naming Conventions**:
- **Raw**: `air_quality_2022.xlsx`, `air_quality_2021.zip`, `wind_2020.xlsx`
- **Staged**: `wind_YYYY.parquet`, `air_quality_{pollutant}_{year}.parquet`
- **Normalised**: `wind_YYYY_normalised.parquet`, `air_quality_{pollutant}_{year}_normalised.parquet`
- **Dimension**: `station_dim.parquet` (shared across datasets)
- **Errors**: `air_quality_2021_failed.txt`

### Processing Architecture

**Airflow DAG Structure** (8 Operational DAGs):
```python
# Phase 3: Data Ingestion
fetch_air_quality (DAG) â†’ 7 parallel tasks (2016-2022)
fetch_wind (DAG) â†’ 5 parallel tasks (2016-2020)

# Phase 4: Staging
dag_stage_wind (DAG) â†’ 5 parallel tasks (2016-2020)
dag_stage_air (DAG) â†’ 4 parallel tasks (2019-2022)

# Phase 4: Normalization
dag_normalise_wind (DAG) â†’ normalise_all_wind task
dag_normalise_air_quality (DAG) â†’ normalise_all_air_quality task

# Phase 5: Analytics (Placeholder)
dag_load_dq_publish (DAG) â†’ Planned for DuckDB loading + DQ checks
```

**Parallel Execution Model**:
- **LocalExecutor**: Up to 32 concurrent tasks
- **Independent Tasks**: Each year processes simultaneously
- **Resource Management**: Automatic load balancing
- **Fault Isolation**: One year's failure doesn't affect others

## Technology Stack

### Core Technologies

**Orchestration**: Apache Airflow 2.10.2
- LocalExecutor for parallel processing
- PostgreSQL metadata storage
- Web UI for monitoring and control

**Data Storage**: DuckDB (embedded analytical database)
- Zero-server footprint
- Excellent performance for analytics workloads
- Portable `.duckdb` files on mounted volumes

**Containerization**: Docker + Docker Compose
- Reproducible deployment across environments
- Automated permission and directory setup
- Health checks and service dependencies

### Python Dependencies

**Core Libraries**:
- `requests`: HTTP client for ArcGIS API integration
- `tenacity`: Retry logic with exponential backoff
- `PyYAML`: Configuration management
- `pandas`: Data manipulation (future phases)
- `duckdb`: Database connectivity (future phases)

**Data Processing** (Future):
- `pyarrow`: Parquet file format support
- `pandera`: Schema validation framework
- `openpyxl`: Excel file processing

## Security Architecture

### Access Control
- **Airflow Admin**: Username/password authentication
- **Container Isolation**: Services run in isolated containers
- **File Permissions**: Proper user/group ownership (airflow:root)

### Data Security
- **Public Data Only**: No sensitive/private data processing
- **Network Security**: HTTPS-only communications with ArcGIS
- **Error Isolation**: Failed downloads quarantined, not exposed

### Secrets Management
- **Environment Variables**: Sensitive config via `.env` files
- **Airflow Secrets**: Database credentials, admin passwords
- **Container Secrets**: AIRFLOW_UID, secret keys

## Performance Characteristics

### Current Performance (Phase 3)

**Download Throughput**:
- Small Excel files: 2-5 seconds each
- Large Excel files: 10-15 seconds each  
- ZIP archives: 15-30 seconds each
- Parallel downloads: 5-12 files simultaneously

**Resource Usage**:
- Memory: ~2GB total (all containers)
- Disk I/O: Streaming downloads (minimal RAM usage)
- Network: Efficient with retry backoff

### Scalability Design

**Horizontal Scaling** (Future):
- CeleryExecutor for distributed task execution
- Redis/RabbitMQ message broker
- Multiple worker nodes

**Vertical Scaling**:
- Increase LocalExecutor parallelism (currently 32)
- Larger container resource allocations
- Faster storage for DuckDB operations

## Monitoring & Observability

### Logging Architecture

**Log Levels**:
- **INFO**: Task start/completion, file downloads
- **WARNING**: Unexpected content types, retries  
- **ERROR**: Download failures, permission issues
- **DEBUG**: Detailed HTTP request/response info

**Log Destinations**:
- **Airflow UI**: Per-task logs with full details
- **Container Logs**: `docker compose logs [service]`
- **File Logs**: `data/logs/` for persistent storage

### Health Monitoring

**Service Health Checks**:
- PostgreSQL: Database connectivity
- Airflow Webserver: HTTP endpoint response
- Container Health: Docker health check status

**Data Pipeline Monitoring** (Future):
- Task success/failure rates per DAG
- Download completion percentages
- Data quality metrics
- Processing time trends

## Error Handling Strategy

### Retry Logic
```python
@retry(
    stop=stop_after_attempt(3),           # Max attempts
    wait=wait_exponential(                # Backoff strategy
        multiplier=1, min=4, max=10
    )
)
```

### Failure Categories

**Network Failures**: Automatic retry with backoff
**HTTP Errors**: Differentiated handling (4xx vs 5xx)
**File System Errors**: Immediate failure with detailed logging
**Validation Errors**: Quarantine with reason logging

### Quarantine System
- **Purpose**: Isolate problematic downloads for manual inspection
- **Content**: Error description, timestamp, source URL
- **Recovery**: Manual intervention or configuration updates

## Completed Architecture (Phase 4)

### âœ… Phase 4: Processing Pipeline COMPLETE
- âœ… **Staging Module**: Excel/ZIP extraction to standardized Parquet (24 files)
- âœ… **Normalization Engine**: Wide-to-tall schema transformation (24 normalised files)
- âœ… **Station Dimension**: Shared idempotent dimension (11 stations, PKs 1-11)
- âœ… **Airflow Orchestration**: 8 operational DAGs for end-to-end processing
- âœ… **Quality Assurance**: 4 EDA notebooks for validation

### ğŸ”„ Phase 5: Analytics & Publishing (Next)
- **DuckDB Loading**: Star schema implementation (`dim_station` + `fact_measurement`)
- **Data Quality Framework**: Pandera validation + domain checks + quality reporting
- **Query Interface**: SQL-based exploration via JupyterLab/DuckDB
- **Cross-Dataset Analytics**: Wind + air quality correlation analysis
- **Export Capabilities**: CSV, Parquet, JSON format support
- **Performance Optimization**: Indexing, partitioning, and caching strategies

This architecture provides a solid foundation for reliable, scalable environmental data processing while maintaining simplicity and maintainability.
