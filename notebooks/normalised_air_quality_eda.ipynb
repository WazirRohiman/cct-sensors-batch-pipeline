{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Air Quality Normalised Data EDA\n",
    "\n",
    "This notebook analyzes the normalised air quality data to verify that the normalisation pipeline worked correctly.\n",
    "\n",
    "## Objectives\n",
    "1. Load and inspect all normalised air quality Parquet files\n",
    "2. Verify tall schema structure and data quality\n",
    "3. Check station mapping consistency with staging\n",
    "4. Analyze temporal coverage and completeness\n",
    "5. Compare pollutant distributions and ranges\n",
    "6. Validate station dimension updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Discover Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "normalised_dir = Path(\"/home/jovyan/work/data/normalised\")\n",
    "config_dir = Path(\"/home/jovyan/work/src/configs\")\n",
    "\n",
    "# Load station mapping configuration\n",
    "with open(config_dir / \"station_mapping.yaml\", 'r') as f:\n",
    "    station_config = yaml.safe_load(f)\n",
    "\n",
    "print(f\"Normalised data directory: {normalised_dir}\")\n",
    "print(f\"Directory exists: {normalised_dir.exists()}\")\n",
    "\n",
    "# Discover all air quality normalised files\n",
    "air_quality_files = list(normalised_dir.glob(\"air_quality_*_normalised.parquet\"))\n",
    "print(f\"\\nFound {len(air_quality_files)} air quality normalised files:\")\n",
    "for file in sorted(air_quality_files):\n",
    "    print(f\"  - {file.name} ({file.stat().st_size / 1024:.1f} KB)\")\n",
    "\n",
    "# Check for station dimension\n",
    "station_dim_path = normalised_dir / \"station_dim.parquet\"\n",
    "print(f\"\\nStation dimension file: {station_dim_path.exists()}\")\n",
    "if station_dim_path.exists():\n",
    "    print(f\"  - station_dim.parquet ({station_dim_path.stat().st_size / 1024:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Station Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load station dimension\n",
    "if station_dim_path.exists():\n",
    "    station_dim = pd.read_parquet(station_dim_path)\n",
    "    print(\"Station Dimension:\")\n",
    "    display(station_dim)\n",
    "    \n",
    "    print(f\"\\nStation dimension shape: {station_dim.shape}\")\n",
    "    print(f\"Unique station PKs: {station_dim['station_pk'].nunique()}\")\n",
    "    print(f\"Station PK range: {station_dim['station_pk'].min()} - {station_dim['station_pk'].max()}\")\n",
    "else:\n",
    "    print(\"❌ Station dimension file not found\")\n",
    "    station_dim = None\n",
    "\n",
    "# Extract air quality metrics from config\n",
    "air_quality_metrics = station_config['air_quality_metrics']\n",
    "pollutant_lookup = {metric['metric_code']: metric for metric in air_quality_metrics}\n",
    "\n",
    "print(\"\\nAir Quality Pollutants:\")\n",
    "for code, metric in pollutant_lookup.items():\n",
    "    print(f\"  {code}: {metric['metric_name']} ({metric['unit']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Analyze All Normalised Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all air quality normalised files\n",
    "air_quality_data = {}\n",
    "file_summary = []\n",
    "\n",
    "for file_path in sorted(air_quality_files):\n",
    "    try:\n",
    "        # Extract pollutant and year from filename\n",
    "        filename = file_path.stem\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) >= 5:  # air_quality_pollutant_year_normalised\n",
    "            pollutant = parts[2]\n",
    "            year = parts[3]\n",
    "        else:\n",
    "            pollutant = 'unknown'\n",
    "            year = 'unknown'\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # Store in dictionary\n",
    "        key = f\"{pollutant}_{year}\"\n",
    "        air_quality_data[key] = df\n",
    "        \n",
    "        # Collect summary info\n",
    "        date_info = \"No datetime\"\n",
    "        if 'datetime' in df.columns:\n",
    "            date_info = (\n",
    "                f\"{df['datetime'].min().date()} to \"\n",
    "                f\"{df['datetime'].max().date()}\"\n",
    "            )\n",
    "        \n",
    "        file_summary.append({\n",
    "            'file': file_path.name,\n",
    "            'pollutant': pollutant,\n",
    "            'year': year,\n",
    "            'rows': len(df),\n",
    "            'columns': len(df.columns),\n",
    "            'date_range': date_info,\n",
    "            'unique_stations': df['station_pk'].nunique() if 'station_pk' in df.columns else 0,\n",
    "            'memory_mb': df.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "        })\n",
    "        \n",
    "        print(f\"✅ Loaded {key}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_path.name}: {e}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = pd.DataFrame(file_summary)\n",
    "print(f\"\\nLoaded {len(air_quality_data)} datasets successfully\")\n",
    "print(f\"Total memory usage: {summary_df['memory_mb'].sum():.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display file summary\n",
    "print(\"File Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Show schema for first file\n",
    "if air_quality_data:\n",
    "    first_key = list(air_quality_data.keys())[0]\n",
    "    first_df = air_quality_data[first_key]\n",
    "    print(f\"\\nSchema for {first_key}:\")\n",
    "    print(first_df.dtypes)\n",
    "    print(f\"\\nSample data:\")\n",
    "    display(first_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate tall schema structure\n",
    "expected_columns = [\n",
    "    'datetime', 'station_pk', 'station_code', 'station_name', \n",
    "    'location_type', 'metric', 'unit', 'value', 'quality_flag', 'source'\n",
    "]\n",
    "\n",
    "print(\"TALL SCHEMA VALIDATION:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "schema_issues = []\n",
    "for key, df in air_quality_data.items():\n",
    "    missing_cols = set(expected_columns) - set(df.columns)\n",
    "    if missing_cols:\n",
    "        schema_issues.append(f\"{key}: Missing columns {missing_cols}\")\n",
    "    \n",
    "    # Check data types\n",
    "    if 'datetime' in df.columns and not pd.api.types.is_datetime64_any_dtype(df['datetime']):\n",
    "        schema_issues.append(f\"{key}: datetime column not datetime type\")\n",
    "    \n",
    "    if 'value' in df.columns and not pd.api.types.is_numeric_dtype(df['value']):\n",
    "        schema_issues.append(f\"{key}: value column not numeric\")\n",
    "\n",
    "if schema_issues:\n",
    "    print(\"❌ Schema issues found:\")\n",
    "    for issue in schema_issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"✅ All files have correct tall schema structure\")\n",
    "\n",
    "print(f\"\\nDATA COMPLETENESS:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check for missing values\n",
    "for key, df in list(air_quality_data.items())[:3]:  # Show first 3 for brevity\n",
    "    null_counts = df.isnull().sum()\n",
    "    total_rows = len(df)\n",
    "    print(f\"\\n{key} ({total_rows:,} rows):\")\n",
    "    for col, null_count in null_counts.items():\n",
    "        if null_count > 0:\n",
    "            percentage = (null_count / total_rows) * 100\n",
    "            print(f\"  {col}: {null_count:,} nulls ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pollutant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all data for analysis\n",
    "if air_quality_data:\n",
    "    all_data = pd.concat(air_quality_data.values(), ignore_index=True)\n",
    "    \n",
    "    print(\"POLLUTANT ANALYSIS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Summary by pollutant\n",
    "    pollutant_summary = (\n",
    "        all_data.groupby('metric')\n",
    "        .agg({\n",
    "            'value': ['count', 'mean', 'std', 'min', 'max'],\n",
    "            'station_pk': 'nunique',\n",
    "            'datetime': ['min', 'max']\n",
    "        })\n",
    "        .round(2)\n",
    "    )\n",
    "    \n",
    "    print(\"Pollutant Statistics:\")\n",
    "    display(pollutant_summary)\n",
    "    \n",
    "    # Quality flag distribution\n",
    "    print(\"\\nQuality Flag Distribution:\")\n",
    "    quality_dist = all_data['quality_flag'].value_counts()\n",
    "    for flag, count in quality_dist.items():\n",
    "        percentage = (count / len(all_data)) * 100\n",
    "        print(f\"  {flag}: {count:,} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "if air_quality_data and len(summary_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Air Quality Normalised Data Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. Records by pollutant and year\n",
    "    pollutant_year_counts = (\n",
    "        summary_df.groupby(['pollutant', 'year'])['rows']\n",
    "        .sum()\n",
    "        .unstack(fill_value=0)\n",
    "    )\n",
    "    pollutant_year_counts.plot(kind='bar', ax=axes[0,0])\n",
    "    axes[0,0].set_title('Records by Pollutant and Year')\n",
    "    axes[0,0].set_xlabel('Pollutant')\n",
    "    axes[0,0].set_ylabel('Number of Records')\n",
    "    axes[0,0].legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Station coverage by pollutant\n",
    "    station_counts = summary_df.groupby('pollutant')['unique_stations'].mean()\n",
    "    station_counts.plot(kind='bar', ax=axes[0,1])\n",
    "    axes[0,1].set_title('Average Station Coverage by Pollutant')\n",
    "    axes[0,1].set_xlabel('Pollutant')\n",
    "    axes[0,1].set_ylabel('Number of Stations')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Data overview text\n",
    "    total_records = summary_df['rows'].sum()\n",
    "    total_files = len(summary_df)\n",
    "    years_covered = sorted(summary_df['year'].unique())\n",
    "    pollutants_covered = sorted(summary_df['pollutant'].unique())\n",
    "    \n",
    "    overview_text = (\n",
    "        f'Total Files: {total_files}\\\\n'\n",
    "        f'Total Records: {total_records:,}\\\\n'\n",
    "        f'Years: {\", \".join(years_covered)}\\\\n'\n",
    "        f'Pollutants: {\", \".join(pollutants_covered)}\\\\n'\n",
    "        f'Schema: Tall Format (VALID)\\\\n'\n",
    "        f'Source: air_quality'\n",
    "    )\n",
    "    axes[1,0].text(\n",
    "        0.5, 0.5, overview_text, \n",
    "        ha='center', va='center', fontsize=12, \n",
    "        transform=axes[1,0].transAxes\n",
    "    )\n",
    "    axes[1,0].set_title('Data Overview')\n",
    "    axes[1,0].axis('off')\n",
    "    \n",
    "    # 4. Memory usage by pollutant\n",
    "    memory_by_pollutant = summary_df.groupby('pollutant')['memory_mb'].sum()\n",
    "    memory_by_pollutant.plot(kind='bar', ax=axes[1,1])\n",
    "    axes[1,1].set_title('Memory Usage by Pollutant')\n",
    "    axes[1,1].set_xlabel('Pollutant')\n",
    "    axes[1,1].set_ylabel('Memory (MB)')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Dataset Compatibility Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for wind normalised files for compatibility\n",
    "wind_files = list(normalised_dir.glob(\"wind_*_normalised.parquet\"))\n",
    "print(f\"Wind normalised files found: {len(wind_files)}\")\n",
    "\n",
    "if wind_files and air_quality_data:\n",
    "    # Load one wind file to compare schema\n",
    "    wind_sample = pd.read_parquet(wind_files[0])\n",
    "    air_sample = list(air_quality_data.values())[0]\n",
    "    \n",
    "    print(\"\\nSCHEMA COMPATIBILITY:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    wind_cols = set(wind_sample.columns)\n",
    "    air_cols = set(air_sample.columns)\n",
    "    \n",
    "    common_cols = wind_cols & air_cols\n",
    "    wind_only = wind_cols - air_cols\n",
    "    air_only = air_cols - wind_cols\n",
    "    \n",
    "    print(f\"Common columns: {len(common_cols)}\")\n",
    "    print(f\"  {sorted(common_cols)}\")\n",
    "    \n",
    "    if wind_only:\n",
    "        print(f\"\\nWind-only columns: {sorted(wind_only)}\")\n",
    "    \n",
    "    if air_only:\n",
    "        print(f\"\\nAir quality-only columns: {sorted(air_only)}\")\n",
    "    \n",
    "    # Check shared stations\n",
    "    wind_stations = set(wind_sample['station_pk'].unique())\n",
    "    air_stations = set(air_sample['station_pk'].unique())\n",
    "    shared_stations = wind_stations & air_stations\n",
    "    \n",
    "    print(f\"\\nSTATION OVERLAP:\")\n",
    "    print(f\"Wind stations: {sorted(wind_stations)}\")\n",
    "    print(f\"Air quality stations: {sorted(air_stations)}\")\n",
    "    print(f\"Shared stations (PKs 1-7): {sorted(shared_stations)}\")\n",
    "    \n",
    "    if len(shared_stations) == 7:\n",
    "        print(\"✅ Expected 7 shared stations found\")\n",
    "    else:\n",
    "        print(f\"⚠️  Expected 7 shared stations, found {len(shared_stations)}\")\n",
    "\n",
    "else:\n",
    "    print(\"Wind data not available for compatibility check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AIR QUALITY NORMALISATION PIPELINE VALIDATION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# File statistics\n",
    "if len(summary_df) > 0:\n",
    "    print(f\"\\n📊 DATA VOLUME:\")\n",
    "    print(f\"  • Total normalised files: {len(air_quality_files)}\")\n",
    "    print(f\"  • Total records: {summary_df['rows'].sum():,}\")\n",
    "    print(f\"  • Total memory usage: {summary_df['memory_mb'].sum():.2f} MB\")\n",
    "    print(f\"  • Years covered: {sorted(summary_df['year'].unique())}\")\n",
    "    print(f\"  • Pollutants processed: {sorted(summary_df['pollutant'].unique())}\")\n",
    "    \n",
    "    # Expected outputs validation\n",
    "    expected_pollutants = ['no2', 'o3', 'pm10', 'pm25', 'so2']\n",
    "    expected_years = ['2019', '2020', '2021', '2022']\n",
    "    \n",
    "    # 2019: 4 files (no PM2.5), 2020-2022: 5 files each\n",
    "    expected_total_files = 4 + (3 * 5)  # 19 files\n",
    "    actual_total_files = len(air_quality_files)\n",
    "    \n",
    "    print(f\"\\n🎯 VALIDATION RESULTS:\")\n",
    "    validation_passed = actual_total_files == expected_total_files\n",
    "    print(f\"  • Expected total files: {expected_total_files}\")\n",
    "    print(f\"  • Actual total files: {actual_total_files}\")\n",
    "    status_text = 'PASSED' if validation_passed else 'FAILED'\n",
    "    print(f\"  • File count validation: {status_text}\")\n",
    "    \n",
    "    # Schema validation\n",
    "    schema_valid = len(schema_issues) == 0\n",
    "    schema_status = 'PASSED' if schema_valid else 'FAILED'\n",
    "    print(f\"  • Tall schema validation: {schema_status}\")\n",
    "    \n",
    "    # Station dimension check\n",
    "    station_dim_valid = station_dim_path.exists()\n",
    "    station_status = 'PASSED' if station_dim_valid else 'FAILED'\n",
    "    print(f\"  • Station dimension present: {station_status}\")\n",
    "    \n",
    "    overall_validation = validation_passed and schema_valid and station_dim_valid\n",
    "    \n",
    "    print(f\"\\n🚀 STATUS:\")\n",
    "    if overall_validation:\n",
    "        print(f\"  • ✅ Air quality normalisation pipeline working correctly\")\n",
    "        print(f\"  • ✅ Tall schema format validated\")\n",
    "        print(f\"  • ✅ Ready for DuckDB loading and cross-dataset analytics\")\n",
    "        print(f\"  • ✅ Station dimension updated with air quality stations\")\n",
    "    else:\n",
    "        print(f\"  • ❌ Review normalisation pipeline for issues\")\n",
    "        if not validation_passed:\n",
    "            print(f\"    - File count mismatch\")\n",
    "        if not schema_valid:\n",
    "            print(f\"    - Schema validation failed\")\n",
    "        if not station_dim_valid:\n",
    "            print(f\"    - Station dimension missing\")\n",
    "    \n",
    "    final_status = 'SUCCESS' if overall_validation else 'NEEDS REVIEW'\n",
    "    print(f\"\\n📝 NORMALISATION PIPELINE STATUS: {final_status}\")\n",
    "    \n",
    "    if overall_validation:\n",
    "        print(f\"\\n🔗 NEXT STEPS:\")\n",
    "        print(f\"  1. Load normalised data into DuckDB\")\n",
    "        print(f\"  2. Combine with wind data for comprehensive analytics\")\n",
    "        print(f\"  3. Implement data quality checks\")\n",
    "        print(f\"  4. Create cross-dataset analysis dashboards\")\n",
    "else:\n",
    "    print(\"\\n❌ No normalised air quality data found\")\n",
    "    print(\"   Run the air quality normalisation pipeline first\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}